defaults:
  - data: default
  - model: model
  - paths: default
  - trainer: default
  - logger: wandb
  - callbacks: default
  - hydra: default
  - _self_
  
## job specific set ups ##
job_name: feature_model
exp_name: feat-anat
#init_method: "tcp://localhost:9992"

device: "cuda:0"
device_generator:
data:
  num_workers: 1

task: feat-anat
dataset_name: {'train': 'synth_id', 'test': 'real_dataset'}

eval_only: False 
debug: False 
freeze_feat: False

resume: False
reset_epoch: False 
resume_optim: True
ckpt_path: /home/tsanchez/Documents/mial/repositories/Brain-ID/outputs/feature_model_feat-anat/2025-05-22/15-07-57/checkpoints/last_train.ckpt

#train_fetal_cfg: '/home/tsanchez/Documents/mial/repositories/FetalSynthGen/configs'

#lr_drops: [70,85]

out_dir: outs/BrainID


supervised_seg_cnn_weights:  
feat_ext_ckp_path:
ckp_path:

## losses and weights

## training params
start_epoch: 0
train_itr_limit: # if not None, it sets the max itr per epoch 
train_subset: #0.2

resume_lr_scheduler: True
n_epochs: 400

## testing params
test_itr_limit: 
test_subset:


seed: 6546
# distributed training parameters
# number of distributed processes
world_size: 1
# url used to set up distributed training
dist_url: env://

## logging intervals
val_epoch: 100000 # must be multiplicable by save_model_epoch

