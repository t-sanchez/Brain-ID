defaults:
  - data: task_seg
  - model: seg_model
  - paths: default
  - trainer: default
  - logger: wandb
  - callbacks: task_seg
  - hydra: default
  - _self_
  
## job specific set ups ##
job_name: seg_run
exp_name: seg

device: "cuda:0"
device_generator:
num_workers: 1

eval_only: False 
debug: False 

resume: False
reset_epoch: False 
resume_optim: True
resume_lr_scheduler: True
freeze_feat: False
load_backbone: True
feat_ckpt: /home/tsanchez/Documents/mial/repositories/Brain-ID/outputs/2025-05-01/18-23-20/checkpoints/last.ckpt
resume_training: False
resume_ckpt: /home/tsanchez/Documents/mial/repositories/Brain-ID/outputs/seg_run_seg/2025-05-07/16-54-23/checkpoints/last.ckpt

## paths 
out_dir: outs/BrainID


supervised_seg_cnn_weights:  
feat_ext_ckp_path:
ckp_path:

## training params
start_epoch: 0
train_itr_limit: # if not None, it sets the max itr per epoch 
train_subset: #0.2

n_epochs: 400


## testing params
test_itr_limit: 
test_subset:


seed: 6546
# distributed training parameters
# number of distributed processes
world_size: 1
# url used to set up distributed training
dist_url: env://


## logging intervals
val_epoch: 100000 # must be multiplicable by save_model_epoch

log_itr: 50
vis_itr: 5000


#log_itr: 20
#vis_itr: 1
